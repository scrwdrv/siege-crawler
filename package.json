{
  "name": "siege-crawler",
  "version": "1.0.8",
  "description": "This CLI tool will find same domain urls in a web page and requesting them to find even more urls until server crash (or at the end of benchmark). It is used to test maximun capacity of server or finding for glitches that users might encounter.",
  "main": "index.js",
  "scripts": {
    "test": "echo \"Error: no test specified\" && exit 1"
  },
  "bin": {
    "siege-crawler": "./index.js"
  },
  "preferGlobal": true,
  "repository": {
    "type": "git",
    "url": "git+https://github.com/scrwdrv/siege-crawler.git"
  },
  "keywords": [
    "cli",
    "tool",
    "siege",
    "ddos",
    "crawler",
    "request",
    "benchmark",
    "debug"
  ],
  "author": "scrwdrv",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/scrwdrv/siege-crawler/issues"
  },
  "homepage": "https://github.com/scrwdrv/siege-crawler#readme",
  "dependencies": {
    "cheerio": "^1.0.0-rc.3",
    "cli-params": "^1.0.6",
    "request": "^2.88.0",
    "simple-regex-toolkit": "^1.0.3"
  },
  "devDependencies": {
    "@types/cheerio": "^0.22.15",
    "@types/node": "^13.1.6",
    "@types/request": "^2.48.4"
  }
}
